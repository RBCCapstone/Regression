{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get features (stops words removed) by tokenizing corpus - no stemming in baseline\n",
    "#Binary encoding\n",
    "#Assign target group \n",
    "#Use mutual information to get final feature set\n",
    "#baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn import datasets\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Padmanie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Padmanie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Padmanie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Padmanie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Padmanie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Testing Feature Selection\n",
    "import nltk\n",
    "\n",
    "## Download Resources\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "from nltk.stem import *\n",
    "\n",
    "# download required resources\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# we'll compare two stemmers and a lemmatizer\n",
    "lrStem = LancasterStemmer()\n",
    "sbStem = SnowballStemmer(\"english\")\n",
    "prStem = PorterStemmer()\n",
    "wnLemm = WordNetLemmatizer()\n",
    "def wnLemm_v(word):\n",
    "    wnLemm = WordNetLemmatizer()\n",
    "    word = wnLemm.lemmatize(word, 'v')\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importData():\n",
    "    #Import Labelled Data\n",
    "    DATA_DIR = \"Data\"\n",
    "    thispath = Path().absolute()\n",
    "    #dtype = {\"index\": str, \"title\": str, \"description\": str, \"url\": str, \"date\": str, \"Retail Relevance\": str, \"Economy Relevant\": str, \"Market moving\": str}\n",
    "    RET_ARTICLES = os.path.join(DATA_DIR, \"Labelled_Articles_.xlsx\")\n",
    "    \n",
    "    df = pd.read_excel(RET_ARTICLES)\n",
    "\n",
    "    try:\n",
    "        df.head()\n",
    "    except:\n",
    "        pass\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignStopWords(): \n",
    "    #Stop_words list Options\n",
    "    #Variation 1: added stop words starting at 'one'\n",
    "    stop_words = stopwords = [\n",
    "        # dates/times\n",
    "        \"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \"december\", \"jan\", \"feb\",\"mar\", \"apr\", \"jun\", \"jul\", \"aug\", \"oct\", \"nov\", \"dec\", \"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\", \"morning\", \"evening\",\n",
    "        # symbols that don't separate a sentence\n",
    "        '$','“','”','’','—',\n",
    "        # specific article terms that are useless\n",
    "        \"read\", \"share\", \"file\", \"'s\",\"i\", \"photo\", \"percent\",\"s\", \"t\", \"inc.\", \"corp\", \"group\", \"inc\", \"corp.\", \"source\", \"bloomberg\", \"cnbc\",\"cnbcs\", \"cnn\", \"reuters\",\"bbc\", \"published\", \"broadcast\",\"york\",\"msnbc\",\n",
    "        # other useless terms\n",
    "        \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"co\", \"inc\", \"com\", \"theyve\", \"theyre\", \"theres\", \"heres\", \"didnt\", \"wouldn\", \"couldn\", \"didn\",\"nbcuniversal\",\"according\", \"just\", \"us\", \"ll\", \"times\"#,\n",
    "        # etc\n",
    "        \"from\",\"the\", \"a\", \"with\", \"have\", \"has\", \"had\", \"having\", \"hello\", \"welcome\", \"yeah\", \"wasn\", \"today\", \"etc\", \"ext\",\"definitely\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"while\", \"of\", \"said\", \"by\", \"for\", \"about\", \"into\", \"through\", \"during\", \"before\", \"after\", \"to\", \"from\", \"in\", \"out\", \"with\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"just\", \"don\", \"now\", \"will\"\n",
    "        ]\n",
    "    #from nltk.corpus import stopwords\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    #print(stop_words)\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_count_words(df, stop_words, text_col = 'content', normalizer=None):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    word_counter = Counter()\n",
    "    for row in df.itertuples(index=True, name='Pandas'):\n",
    "            attribute = str((row, text_col))\n",
    "            file_words = tokenizer.tokenize(attribute)\n",
    "            #keep lowercased words that are not stop words as features\n",
    "            file_wordsNS = [word.lower() for word in file_words if not word.lower() in stop_words]\n",
    "            # remove words that are numbers\n",
    "            file_wordsN = [word for word in file_wordsNS if not word.isnumeric()]\n",
    "            #remove words with a word length less than 4 (i.e. 1-3)\n",
    "            file_wordsF = [word for word in file_wordsN if not len(word)<4]\n",
    "            \n",
    "            #stem\n",
    "            if normalizer:\n",
    "                file_wordsF = [normalizer(word) for word in file_wordsF]\n",
    "            \n",
    "            word_counter.update(file_wordsF)\n",
    "    return word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary encoding for features, also appends retail target group\n",
    "def binary_encode_features(newsarticles, top_words, text_col = 'content', normalizer=None):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    df_rows = []\n",
    "    for row in tqdm(newsarticles.itertuples(index=True, name='Pandas')):\n",
    "            attribute = str((row, text_col))\n",
    "            file_words = tokenizer.tokenize(attribute)\n",
    "            if normalizer:\n",
    "                file_words = [normalizer(word) for word in file_words]\n",
    "            df_rows.append([1 if word.lower() in file_words else 0 for word in top_words])      \n",
    "    X = pd.DataFrame(df_rows, columns = top_words)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutualInformation(B_Encoding, y, top_words): \n",
    "    #Estimate mutual information for a discrete target variable.\n",
    "    #Mutual information (MI) [1] between two random variables is a non-negative value, which measures the dependency between the variables.\n",
    "    #It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n",
    "    featureVals= mutual_info_classif(B_Encoding, y, discrete_features='auto', n_neighbors=3, copy=True, random_state=None)\n",
    "    \n",
    "    np.asarray(featureVals)\n",
    "\n",
    "    Temp= pd.DataFrame(featureVals, columns = ['MI_Values'])\n",
    " \n",
    "    Final = Temp.assign(target_group = top_words)\n",
    "    \n",
    "    Highest_Features = Final.nlargest(10000, 'MI_Values')\n",
    "    \n",
    "    return Highest_Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectFeatures(text_col = 'content', **kwargs):\n",
    "    df = importData()\n",
    "    df.columns\n",
    "    stop_words = assignStopWords()\n",
    "    \n",
    "    if ('norm' in kwargs):\n",
    "        norm = kwargs['norm']\n",
    "        normalizers = {'lrStem' : lrStem.stem,\n",
    "                       'sbStem' : sbStem.stem,\n",
    "                       'prStem' : prStem.stem,\n",
    "                       'wnLemm' : wnLemm.lemmatize,\n",
    "                       'wnLemm-v':wnLemm_v,\n",
    "                       'baseline':None\n",
    "                      }\n",
    "        normalizer = normalizers[norm]\n",
    "    \n",
    "    #Select subset of orig data\n",
    "    #print(df.head(2))\n",
    "    df1 = df[[text_col,'market_moving']]    \n",
    "    news_cnt = corpus_count_words(df1, stop_words, text_col = text_col, normalizer = normalizer)\n",
    "    \n",
    "    print(\"starting Binary Encoding\")\n",
    "    num_features = 1000\n",
    "    top_words = [word for (word, freq) in news_cnt.most_common(num_features)]\n",
    "    B_Encoding = binary_encode_features(df1, top_words, text_col = text_col, normalizer = normalizer)\n",
    "    print(B_Encoding.head())\n",
    "    y = df['market_moving']\n",
    "    B_Encoding.assign(target_group=y)\n",
    "      \n",
    "    print(\"Finished Bin Encoding. Collecting Highest Features\")\n",
    "    Highest_Features = mutualInformation(B_Encoding, y, top_words)\n",
    "    Highest_Features = pd.DataFrame(Highest_Features)\n",
    "    \n",
    "    # Save as csv file in DATACOLLECTION data folder (bc it's needed for encoding script)\n",
    "    if ('csv' in kwargs) and (kwargs['csv']):\n",
    "        \n",
    "        # File path for this file\n",
    "        file_name = norm + text_col + 'FeatureSet.csv'\n",
    "        thispath = Path().absolute()\n",
    "        OUTPUT_DIR = os.path.join(thispath, \"Data\", file_name)\n",
    "        \n",
    "        # if the following line throws an error, use the line after to save in same folder\n",
    "        pd.DataFrame.to_csv(Highest_Features, path_or_buf=OUTPUT_DIR)\n",
    "        #pd.DataFrame.to_csv(Highest_Features, path_or_buf=file_name)\n",
    "    \n",
    "    print(Highest_Features)\n",
    "    return Highest_Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    HF = selectFeatures(csv = True, )\n",
    "    return HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: wnLemm\n",
      "starting Binary Encoding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3330it [00:02, 1131.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   title  index  pandas  market_moving  amazon  tech  stocks  apple  wall  \\\n",
      "0      1      0       0              1       0     0       0      0     0   \n",
      "1      1      0       0              1       0     0       0      0     0   \n",
      "2      1      0       0              1       0     0       0      0     0   \n",
      "3      1      0       0              1       0     0       0      0     0   \n",
      "4      1      0       0              1       0     0       0      0     0   \n",
      "\n",
      "   street   ...     summer  simple  warehouse  teams  atul  gawande  every  \\\n",
      "0       0   ...          0       0          0      0     0        0      0   \n",
      "1       0   ...          0       0          0      0     0        0      0   \n",
      "2       0   ...          0       0          0      0     0        0      0   \n",
      "3       0   ...          0       0          0      0     0        0      0   \n",
      "4       0   ...          0       0          0      0     0        0      0   \n",
      "\n",
      "   cctv  script  playing  \n",
      "0     0       0        0  \n",
      "1     0       0        0  \n",
      "2     0       0        0  \n",
      "3     0       0        0  \n",
      "4     0       0        0  \n",
      "\n",
      "[5 rows x 1000 columns]\n",
      "Finished Bin Encoding. Collecting Highest Features\n",
      "     MI_Values target_group\n",
      "427   0.016959        month\n",
      "573   0.016770         wins\n",
      "835   0.016585         walk\n",
      "501   0.016227       postal\n",
      "779   0.015517    announces\n",
      "237   0.014987       people\n",
      "465   0.014986       buyers\n",
      "718   0.014790       effect\n",
      "780   0.014486     victoria\n",
      "936   0.014445     tensions\n",
      "578   0.014366      reports\n",
      "531   0.014344        homes\n",
      "977   0.014146       shadow\n",
      "508   0.014008        wages\n",
      "993   0.013872        teams\n",
      "236   0.013795       europe\n",
      "265   0.013690        faces\n",
      "479   0.013342         exec\n",
      "187   0.013149        costs\n",
      "860   0.013053       action\n",
      "742   0.013010      twitter\n",
      "418   0.013002      startup\n",
      "839   0.012967     medicaid\n",
      "193   0.012649      looking\n",
      "816   0.012583         mean\n",
      "912   0.012571     shooting\n",
      "634   0.012538       owners\n",
      "576   0.012387      rebound\n",
      "36    0.012314     earnings\n",
      "155   0.012244        would\n",
      "..         ...          ...\n",
      "475   0.000000        goods\n",
      "476   0.000000         rate\n",
      "478   0.000000        fedex\n",
      "480   0.000000        chain\n",
      "482   0.000000    continues\n",
      "485   0.000000    democrats\n",
      "486   0.000000        offer\n",
      "490   0.000000         four\n",
      "491   0.000000         nike\n",
      "454   0.000000       winner\n",
      "453   0.000000    employers\n",
      "452   0.000000      members\n",
      "426   0.000000       school\n",
      "411   0.000000        means\n",
      "412   0.000000         jump\n",
      "415   0.000000       points\n",
      "416   0.000000        worst\n",
      "417   0.000000       double\n",
      "420   0.000000     congress\n",
      "422   0.000000     shipping\n",
      "428   0.000000         ceos\n",
      "449   0.000000         self\n",
      "429   0.000000     spending\n",
      "431   0.000000        brand\n",
      "432   0.000000     softbank\n",
      "434   0.000000         rest\n",
      "440   0.000000        early\n",
      "442   0.000000       closes\n",
      "448   0.000000        stake\n",
      "999   0.000000      playing\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "content: wnLemm\n",
      "starting Binary Encoding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3330it [00:49, 67.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year  company  content  amazon  would  companies  market  like  people  \\\n",
      "0     1        1        1       0      1          0       0     1       1   \n",
      "1     1        0        1       0      1          0       1     0       0   \n",
      "2     1        1        1       0      1          0       0     0       1   \n",
      "3     1        1        1       1      1          1       0     1       1   \n",
      "4     0        0        1       1      1          1       0     0       1   \n",
      "\n",
      "   says   ...     leaving  continues  opening  shop  german  expects  pace  \\\n",
      "0     0   ...           0          0        0     0       0        0     0   \n",
      "1     1   ...           0          0        0     0       0        0     0   \n",
      "2     0   ...           0          0        0     0       0        0     0   \n",
      "3     0   ...           0          0        0     0       0        0     0   \n",
      "4     0   ...           0          0        0     0       0        0     0   \n",
      "\n",
      "   faster  review  friends  \n",
      "0       0       0        1  \n",
      "1       0       0        0  \n",
      "2       0       0        0  \n",
      "3       0       0        0  \n",
      "4       0       0        0  \n",
      "\n",
      "[5 rows x 1000 columns]\n",
      "Finished Bin Encoding. Collecting Highest Features\n",
      "     MI_Values  target_group\n",
      "112   0.042625        stores\n",
      "123   0.031809        retail\n",
      "512   0.028014      shoppers\n",
      "25    0.027535         sales\n",
      "356   0.025549      retailer\n",
      "473   0.022239         items\n",
      "124   0.022009         store\n",
      "327   0.020588        school\n",
      "610   0.020502          side\n",
      "698   0.018926        russia\n",
      "96    0.018244       walmart\n",
      "182   0.018241     retailers\n",
      "92    0.017567          good\n",
      "116   0.017428         might\n",
      "883   0.016653      received\n",
      "56    0.016436        online\n",
      "782   0.016387          hand\n",
      "369   0.016226         chain\n",
      "74    0.016176     customers\n",
      "597   0.015732         range\n",
      "555   0.015501   acquisition\n",
      "256   0.015167     important\n",
      "224   0.015140     political\n",
      "536   0.015138          huge\n",
      "775   0.014852       compete\n",
      "15    0.014779          time\n",
      "447   0.014730          fact\n",
      "187   0.014639       without\n",
      "652   0.014525    previously\n",
      "106   0.014457       already\n",
      "..         ...           ...\n",
      "654   0.000000        owners\n",
      "147   0.000000          sell\n",
      "689   0.000000      vehicles\n",
      "657   0.000000          jeff\n",
      "688   0.000000       forward\n",
      "687   0.000000        leader\n",
      "686   0.000000    commission\n",
      "683   0.000000         gains\n",
      "682   0.000000    additional\n",
      "680   0.000000         class\n",
      "679   0.000000         allow\n",
      "249   0.000000     announced\n",
      "677   0.000000        season\n",
      "395   0.000000      personal\n",
      "675   0.000000        effect\n",
      "674   0.000000         union\n",
      "673   0.000000       website\n",
      "672   0.000000         reach\n",
      "671   0.000000  technologies\n",
      "670   0.000000          stop\n",
      "669   0.000000          talk\n",
      "668   0.000000        amount\n",
      "397   0.000000         clear\n",
      "665   0.000000       decline\n",
      "664   0.000000     difficult\n",
      "142   0.000000       largest\n",
      "662   0.000000        invest\n",
      "145   0.000000      research\n",
      "659   0.000000       goldman\n",
      "500   0.000000        issues\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "nrms = ['wnLemm-v', 'lrStem', 'sbStem', 'prStem', 'wnLemm']\n",
    "txtcols = ['title', 'content']\n",
    "\n",
    "for txtcol in txtcols:\n",
    "    #for nrm in nrms:\n",
    "    nrm = nrms[4]\n",
    "    print(txtcol + ': ' + nrm)\n",
    "    HF = selectFeatures(text_col = txtcol, norm = 'baseline', csv=True, )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'normalizer' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-cb9946b92fd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mHighest_Features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Let Paddy know the code is done:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwinsound\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mduration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m  \u001b[1;31m# millisecond\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-86c4a22da8cd>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mHF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselectFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mHF\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-bf069bb39924>\u001b[0m in \u001b[0;36mselectFeatures\u001b[1;34m(text_col, **kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m#print(df.head(2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext_col\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'market_moving'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mnews_cnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus_count_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"starting Binary Encoding\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'normalizer' referenced before assignment"
     ]
    }
   ],
   "source": [
    "Highest_Features = main()\n",
    "\n",
    "# Let Paddy know the code is done:\n",
    "import winsound\n",
    "duration = 1000  # millisecond\n",
    "freq = 440  # Hz\n",
    "winsound.Beep(freq, duration)\n",
    "winsound.Beep(600, 500)\n",
    "winsound.Beep(freq, duration)\n",
    "winsound.Beep(600, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureSet = pd.DataFrame(Highest_Features['target_group'])\n",
    "    \n",
    "# Save as csv file in DATACOLLECTION data folder (bc it's needed for encoding script)\n",
    "\n",
    "\n",
    "# File path for this file\n",
    "file_name = 'retailFeatureSet.csv'\n",
    "thispath = Path().absolute()\n",
    "OUTPUT_DIR = os.path.join(thispath, \"Data\", file_name)\n",
    "\n",
    "# if the following line throws an error, use the line after to save in same folder\n",
    "pd.DataFrame.to_csv(featureSet, path_or_buf=OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(Highest_Features['MI_Values'].values)\n",
    "plt.ylabel('MI Score')\n",
    "plt.axis([0, 250, 0, 0.16])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Highest_Features['MI_Values'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Highest_Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
