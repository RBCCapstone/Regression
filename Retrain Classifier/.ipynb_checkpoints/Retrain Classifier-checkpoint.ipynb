{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this notebook to train the classifier for new data. \n",
    "Just make sure that you put the new data into the Data folder and name it 'Labelled Data.xlsx'  \n",
    "Then all you have to do is run through each of the following cells to create the new classifier.  \n",
    "Once that's created, copy the files over for the pipeline:  \n",
    "* OurClassifier.p --> Pipeline\n",
    "* content-wnLemm-FeatureSet.csv --> Pipeline/Data\n",
    "* title-wnLemm-FeatureSet.csv --> Pipeline/Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Clean the Data\n",
    "\n",
    "The first step cleans the articles and preps them for the rest of the classifier training.  \n",
    "Along with other cleaning functions, this script removes:\n",
    "* duplicate articles\n",
    "* invalid 'articles' (e.g. \"Your usage has been flagged\", Chinese characters, etc.)  \n",
    "\n",
    "*From articles:*\n",
    "* html tags (e.g. 'div', etc.)\n",
    "* stop words (e.g. 'and', 'the', dates, prepositions, etc.)\n",
    "* standard phrases (e.g. 'click here', 'read more') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DataClean as dc\n",
    "\n",
    "articleDB = dc.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Select Features\n",
    "This script runs to select the most valuable words out of the article set that can help the classifier determine whether an article is market moving or not.  \n",
    "Essentially, this script normalizes words and then selects the top 1000 according to the Mutual Information metric.\n",
    "\n",
    "Because both the article body and title provide a lot of information, this script runs for both of those datasets and outputs two feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Padmanie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Padmanie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Padmanie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Padmanie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "starting Binary Encoding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2228it [00:00, 2599.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Bin Encoding. Collecting Highest Features\n",
      "     MI_Values    target_group\n",
      "833   0.020135       regulator\n",
      "305   0.019532          winner\n",
      "800   0.019016      revolution\n",
      "303   0.018799            hold\n",
      "96    0.018552        briefing\n",
      "695   0.018299           music\n",
      "269   0.018228            save\n",
      "257   0.018132          rising\n",
      "25    0.018127            year\n",
      "965   0.017988            ripe\n",
      "244   0.017538        industry\n",
      "917   0.017254           fresh\n",
      "98    0.016643            work\n",
      "617   0.016491        explains\n",
      "530   0.016397            must\n",
      "465   0.016351  cryptocurrency\n",
      "310   0.016340            elon\n",
      "72    0.015912            sell\n",
      "636   0.015793            perk\n",
      "473   0.015578         deficit\n",
      "255   0.015539            game\n",
      "494   0.015494           avoid\n",
      "320   0.015160          energy\n",
      "464   0.015135            lead\n",
      "245   0.014777          secret\n",
      "243   0.014552        trillion\n",
      "295   0.014089         venture\n",
      "577   0.013745        broadcom\n",
      "672   0.013729           space\n",
      "763   0.013634          couple\n",
      "..         ...             ...\n",
      "234   0.000000            mark\n",
      "235   0.000000            asia\n",
      "236   0.000000        shopping\n",
      "237   0.000000             get\n",
      "633   0.000000        happened\n",
      "238   0.000000        customer\n",
      "629   0.000000            tool\n",
      "627   0.000000            macy\n",
      "626   0.000000          nvidia\n",
      "241   0.000000            chip\n",
      "246   0.000000        pressure\n",
      "622   0.000000         rebound\n",
      "621   0.000000           level\n",
      "619   0.000000            wine\n",
      "602   0.000000           brace\n",
      "248   0.000000       investing\n",
      "616   0.000000            ramp\n",
      "615   0.000000          kroger\n",
      "249   0.000000         benefit\n",
      "613   0.000000          equity\n",
      "250   0.000000          double\n",
      "251   0.000000          budget\n",
      "252   0.000000       president\n",
      "253   0.000000            free\n",
      "608   0.000000            blue\n",
      "607   0.000000         support\n",
      "254   0.000000          former\n",
      "262   0.000000           major\n",
      "263   0.000000           video\n",
      "999   0.000000            room\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "starting Binary Encoding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2228it [00:16, 136.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Bin Encoding. Collecting Highest Features\n",
      "     MI_Values    target_group\n",
      "97    0.040577        retailer\n",
      "41    0.028876           store\n",
      "154   0.028782          retail\n",
      "613   0.027383  infrastructure\n",
      "498   0.026625         shopper\n",
      "408   0.022393        declined\n",
      "684   0.022369          disney\n",
      "436   0.021520          agency\n",
      "842   0.020074          author\n",
      "633   0.019995      california\n",
      "23    0.019614            sale\n",
      "159   0.019408            used\n",
      "952   0.019239        original\n",
      "141   0.019181           house\n",
      "58    0.018724            high\n",
      "348   0.018098          always\n",
      "955   0.017832       secretary\n",
      "274   0.017770           chain\n",
      "223   0.017527        question\n",
      "877   0.017382         reached\n",
      "3     0.016860          amazon\n",
      "911   0.016598         raising\n",
      "336   0.016530         getting\n",
      "7     0.016520            like\n",
      "474   0.016477      republican\n",
      "91    0.016047            city\n",
      "193   0.015982         growing\n",
      "298   0.015937          europe\n",
      "246   0.015800          social\n",
      "522   0.015755       agreement\n",
      "..         ...             ...\n",
      "501   0.000000           along\n",
      "502   0.000000           shift\n",
      "504   0.000000          amount\n",
      "507   0.000000          bought\n",
      "508   0.000000          figure\n",
      "509   0.000000           event\n",
      "510   0.000000        electric\n",
      "512   0.000000           bring\n",
      "514   0.000000      especially\n",
      "516   0.000000          charge\n",
      "517   0.000000          iphone\n",
      "489   0.000000        actually\n",
      "485   0.000000           force\n",
      "481   0.000000           south\n",
      "467   0.000000           daily\n",
      "458   0.000000           email\n",
      "459   0.000000       following\n",
      "462   0.000000          parent\n",
      "463   0.000000         speaker\n",
      "464   0.000000          donald\n",
      "466   0.000000         fortune\n",
      "470   0.000000        contract\n",
      "480   0.000000         decline\n",
      "471   0.000000         outside\n",
      "472   0.000000           smart\n",
      "473   0.000000            card\n",
      "477   0.000000           watch\n",
      "478   0.000000             way\n",
      "479   0.000000           stake\n",
      "999   0.000000          reduce\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import FeatureSelection as fs\n",
    "\n",
    "titleFts, contentFts = fs.main(articleDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Encode Features\n",
    "\n",
    "Now that the top features have been selected, each article is encoded into a vector according to the features that it contains.\n",
    "\n",
    "In other words, this script creates a matrix where each row represents an article (i) and each column is a selected feature (j). A cell receives a 1 if the feature (i.e. word) j appears in article i, otherwise it receives a 0. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Padmanie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Padmanie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Padmanie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Padmanie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "encoding title features\n",
      "encoding content features\n"
     ]
    }
   ],
   "source": [
    "import FeatureEncoding as fe\n",
    "\n",
    "titleEnc, contentEnc = fe.main(titleFts, contentFts, articleDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create a new logistic regression classifier\n",
    "\n",
    "Finally, this script takes in the encoded articles and feature sets to combine them into a single matrix.\n",
    "This matrix is fed into a logistic regression classifier and tested with various combinations of hyperparameters.\n",
    "\n",
    "Based on the results of this testing (hyperparameter tuning), the top combination for linear regression is run and stored as a new classifier, \"OurClassifier.p\".\n",
    "\n",
    "This pickle file contains logistic regression classifier, including hyperparameters and feature weights, that can be used regularly on the new data that the Pipeline pulls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2228, 351)\n",
      "RangeIndex(start=0, stop=2228, step=1)\n",
      "Best Penalty: l2 Best C: 1\n",
      "Best Penalty: l2 Best C: 0.1\n",
      "Best Penalty: l2 Best C: 0.01\n",
      "Best Penalty: l1 Best C: 0.1\n",
      "Best Penalty: l2 Best C: 0.1\n",
      "0.7169249471928401\n",
      "0.8534286873605194\n"
     ]
    }
   ],
   "source": [
    "import LogisticRegression as lr\n",
    "\n",
    "results = lr.main(contentEnc, titleEnc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
