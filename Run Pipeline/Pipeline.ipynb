{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline to run all files\n",
    "\n",
    "* Our plan is to not store anything (refresh daily) but need to consider how to deal with taking in user feedback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pull Articles\n",
    "\n",
    "This script uses NewsAPI to pull a set of articles.\n",
    "These articles are filtered to only come from reputable news sources (top 19) and must contain a mention of at least one of the top 20 retail companies, to ensure that we have reliable and retail-related articles.\n",
    "\n",
    "By default the script pulls articles over the last week. However, if you'd like to specify a date range, the script takes in:\n",
    "* pull_from = date in \"YYYY-MM-DD\" format\n",
    "* pull_to   = date in \"YYYY-MM-DD\" format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering articles on (Gap Inc) OR (Foot Locker) OR (L Brands) OR Macerich OR Kimco OR TJX OR CVS OR (Home Depot) OR (Best Buy) OR (Lowe's) OR Walmart OR (Target's) OR TGT OR Amazon OR Kroger OR Walgreens OR Kohl's OR (Dollar General) OR (Bed Bath and Beyond) OR Safeway from: 2019-04-10 to 2019-04-16\n",
      "885\n"
     ]
    }
   ],
   "source": [
    "# Pull news articles with NewsAPI\n",
    "import NewsAPI as news\n",
    "\n",
    "# optional inputs: pull_from, pull_to\n",
    "# format \"YYYY-MM-DD\"\n",
    "# where pull_to > pull_from\n",
    "articleDB = news.main() #output is called 'NewsAPIOutput.xlsx' in Python Scripts > Data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Clean Articles\n",
    "\n",
    "The step cleans the articles and preps them for the rest of the Pipeline. \n",
    "\n",
    "Along with other cleaning functions, this script removes:\n",
    "* duplicate articles\n",
    "* invalid 'articles' (e.g. \"Your usage has been flagged\", Chinese characters, etc.)  \n",
    "* articles with less than 300 words\n",
    "\n",
    "*From articles:*\n",
    "* html tags (e.g. 'div', etc.)\n",
    "* stop words (e.g. 'and', 'the', dates, prepositions, etc.)\n",
    "* standard phrases (e.g. 'click here', 'read more') \n",
    "\n",
    "It stores three copies of the data:\n",
    "1. For interface: stripped of only tags, links, and standard phrases, maintaining punctuation and capitalization for readability (for final output)\n",
    "2. For keyphrase tagging: all of (1) + stripped of markup, time, url, punctuation that isn't associated with stops (e.g. quotation marks)\n",
    "3. For classification: all of (1) + (2) + stripped of all punctuation, capitalization, stop words etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Article Cleaning (must pip install tqdm first (only once) to run)\n",
    "import dataClean as dc\n",
    "\n",
    "articleDB = dc.DataClean(articleDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Encode Features for Classifier\n",
    "\n",
    "This script takes the list of features stored as .csv in the Data subfolder to encode features for the classifier.\n",
    "Based on this list of features, each article is encoded into a vector according to the features that it contains.\n",
    "\n",
    "In other words, this script creates a matrix where each row represents an article (i) and each column is a selected feature (j). A cell receives a 1 if the feature (i.e. word) j appears in article i, otherwise it receives a 0. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Padmanie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Padmanie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Padmanie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "content\n",
      "Binary Encoding\n",
      "title\n",
      "Binary Encoding\n"
     ]
    }
   ],
   "source": [
    "#Feature Selection and Binary Article Encoding\n",
    "import FeatureEncoding as fe\n",
    "contentBinaryMatrix = fe.encoding(0, df=articleDB, text_col='content', norm='wnLemm')\n",
    "titleBinaryMatrix = fe.encoding(0, df=articleDB, text_col = 'title', norm='wnLemm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Classify articles as market moving or not (ML Algo 1)\n",
    "\n",
    "Finally Classifying!\n",
    "This script predicts whether each new article is market moving or not, based on the pre-trained logistic regression classifier. It takes in the encoded matrices and appends a its predictions as a column to the articleDB dataframe.\n",
    "\n",
    "It also ranks articles based on the logistic regression prediction from most likely to be market moving to least likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression Classifier + Article Ranking, complete final file is called 'results_encoding.xlsx'\n",
    "import logReg as lr\n",
    "articleDB = lr.runLogReg(titleBinaryMatrix, contentBinaryMatrix, articleDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Extract Article Tags and Trending Terms (ML Algo 2)\n",
    "\n",
    "Each article is tagged with 5 key phrases that help to identify the context within the article.  \n",
    "This script extracts the key phrases from each article and ranks them by Pointwise Mutual Information (how useful it is), through a Content Extraction algorithm.  \n",
    "These tags are then displayed along article headlines in the interface and highlighted within the article text.  \n",
    "\n",
    "Inputs: \n",
    "    1. articleDB - uses column 'content' \n",
    "    2. (optional) - tag type (could be 'ngrams'{unlimited}, 'bigrams'{terms with up to 2 words}, or 'unigrams'{single terms}) \n",
    "        * default is 'bigrams'\n",
    "Outputs:\n",
    "    1. articleDB = articleDB with appended columns `tags` and `tags_top_5`\n",
    "    2. trendingTermsDB = keyterms by # article mentions\n",
    "    \n",
    "* Note: `tags` currently stores more tags than is probably helpful. Quick fix: adjust ranking code in ContextExtraction.py to output top 15-20 tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 555/555 [00:17<00:00, 31.31it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 555/555 [00:53<00:00, 12.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code extracts and ranks \"tags\" + counts frequency of tag mentions in articles \n",
    "import ContextExtraction as ce\n",
    "articleDB, trendingTermsDB = ce.retrieveContext(articleDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Recommend Related Articles (ML Algo 3)\n",
    "\n",
    "In order to allow FAs to further explore a topic of interest, this algorithm provides the top three similar articles to any given article. These articles are ranked by similarity to an article, regardless of whether they are market moving or not.\n",
    "\n",
    "This script also appends a column of related article ID's to the ArticleDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tifidf Encoding\n",
      "bin Encoding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 556/556 [00:01<00:00, 492.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf Encoding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 556/556 [00:01<00:00, 472.95it/s]\n"
     ]
    }
   ],
   "source": [
    "import Recommender as rec\n",
    "articleDB = rec.recommender(articleDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Output the Interface Data\n",
    "\n",
    "This final script consolidates all of the information gathered above into a single json file that is displayed through the RBC interface. The final output is called `data.json` which can be dragged into the the Git for the interface built by Hayden and the kind co-op devs at RBC!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import frontPage as fp\n",
    "frontpage = fp.FrontPage(articleDB, trendingTermsDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
